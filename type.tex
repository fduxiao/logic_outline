\chapter{Type Theory}
\label{type-theory}

In this chapter, I will describe a formal language like the language of 
set theory. Recall we develop the set theory with first order language,
where every axiom represents a kind of set operation or property. Despite
the syntax of first order language, we tend to use semantic proofs. For
example, try to prove ``a group of order prime squared is abelian''.
You will naturally pick the order(size) of the group, which is certainly
not expressible in the first order group theory. 

From the perspective of first order language, we define how the set 
(model) structure is formed, while in real world, we perform the 
computation (of sets) directly. If you know how the computation is made, 
you must be able to write down the proof (though it may be very difficult, 
as the example above). Now think what is the first order language and 
its inference rules. In mathematical logic, the language itself is 
defined as an object (a set), and the rules are defined as computations 
(functions). (Note both the {\it modus ponens} and axioms 
represent whether you are allowed to introduce or eliminate certain 
sentences. Just think them as buttons on a calculator.) Again, the axioms 
in ZFC \footnote{when ZFC is considered as the identical but distinct 
version of the one we use in meta language} are also such rules. In 
summary, the logic language is the object, while our inference is doing 
computations just like applying a function to an element to derive another. 

This coincidence (the inference rules can be viewed as 
object operations) is called {\it Curry-Howard 
Correspondence}. But instead of set theory, typed lambda calculus
\cite{Curry-Howard} is chosen for it, mainly because it's more natural 
in computer programming \cite{coq,SF}, and some syntactic rule of lambda 
calculus coincides with modus ponens, so you won't have to treat the 
first order language specially
(one layer of deduction system and syntax and another layer of semantics). 
Look at the group above, the semantic proof is very natural in our 
language, but is not applicable pure syntactically. In the rest of this 
chapter I'm going to introduce the type theory (mainly a re-illustration 
of the first chapter of \cite{homotopy-type-theory}). Bear in mind that 
everything is associated with a construction and under such a scheme, 
no extra relation between the first order layer and the semantic layer
is needed. 

\section{Lambda-Calculus}
The core of our language is the lambda calculus. I first start with an
untyped version, and then give a simply-typed version. You don't
have to think about its real meaning. Just play with it until you are
happy with this concept. (See chapter 1, 3 of \cite{Curry-Howard})

Let $V=\{v_0, v_1, \cdots\}$ be a variable set\footnote{The only
requirement for this set is that it should be infinite. Though many
may choose a countably infinite one for convenience, it is more common
for computer programs to use a `free monoid generated by finitely many
characters', i.e. strings like $a,ab,abc,xyz$}.  We defines
{\it$\lambda$-terms} as follows\footnote{This notation is
called the backus normal form, or see \autoref{classical-logic}.}. 

\begin{definition}\label{lambda-term}($\lambda$-terms)
    $$\Lambda ::= V \mid (\Lambda\ \Lambda) \mid (\lambda V\ \Lambda)$$
\end{definition}

You can understand it as:
\begin{enumerate}
    \item A variable is a $\lambda$-$term$.
    \item If $T_1$ and $T_2$ are $\lambda$-$terms$, then so is $(T_1\ T_2)$.
    This is called an application.
    \item If $x\in V$ is a variable, $T$ is a $\lambda$-$term$, then so
    is $(\lambda x\ T)$. This is called an abstraction. 
\end{enumerate}

Here are some simple examples of $\lambda$-terms.
\begin{enumerate}
    \item $x$
    \item $(M\ N)$
    \item $(\lambda x\  x)$
    \item $(\lambda x\ (y\ (\lambda z\ (x\ z))))$
\end{enumerate}

Informally, the abstraction is like the analytic form of a function.
For example, the function $f(x) = x^2 + 1$ can be expressed with 
abstraction $f=(\lambda x\ (x^2+1))$. ($2,+,1$ will be introduced later.)
The application is also analogous to function application, e.g. 
$(f\ 2)=((\lambda x\ (x^2+1))\ 2)$. As you may guess, we can `compute' 
this $\lambda$-term with purely substitution: 
$(f\ 2)=((\lambda x\ (x^2+1))\ 2)\to (2^2+1)$, but before the formal
definition of this process, we have to handle the problem of
different choices of the variable $x$ next to $\lambda$ (bounded variable). 

\

We first make the following conventions to save parentheses.

\begin{enumerate}
    \item $(\lambda x\ \lambda y\ T)$ is $(\lambda x\ (\lambda y\ T))$.
    \item $(K\ L\ M)$ is $((K\ L)\ M)$. 
    \item $(\lambda x\ M\ N)$ is $(\lambda x\ (M\ N))$.
    \item $(M\ \lambda x\ N)$ is $(M\ (\lambda x\ N))$.
    \item The outermost parentheses are also omitted if that won't cause ambiguity. 
\end{enumerate}

\begin{remark}
    A function $f:X\times Y\to Z$ can be viewed as a function
    sending each $x\in X$ to a function $Y\to Z$, which is the
    exponential rule in category theory\cite{cat-awodey}. In 
    $\lambda$-calculus, this style is called {\it currying}.
    It is then reasonable to consider $(\lambda x\ \lambda y\ T)$
    as $(\lambda x\ (\lambda y\ T))$. In response, the application
    is left associative, and we also write 
    $\lambda x_1\cdots x_n.M$ for 
    $(\lambda x_1\ (\lambda x_2\ (\cdots(\lambda x_n\ M))))$.
\end{remark}



\newcommand{\FV}{{\text{FV}}}
\begin{definition}
    (Free Variable) For $M\in\Lambda$, we define a set $\FV(M)\subseteq V$
    of {\it free variables} recursively.
    \eq{
        \FV(x) & = \{x\} \\
        \FV(\lambda x.P) &= \FV(P)\setminus\{x\} \\
        \FV(P\ Q) &= \FV(P)\cup \FV(Q)
    }
\end{definition}

\begin{example}
    \
    \begin{enumerate}
        \item $\FV(a\ b)=\{a,b\}$
        \item $\FV(\lambda x.x\ y)=\{y\}$
    \end{enumerate}
\end{example}

The function $f(x)=x^2+1$ can also be written as $f(y)=y^2+1$. These two
representations are considered the same\footnote{Formally speaking, you
can define the terms as equivalence classes under this equality, but the
free variables will still be a trouble. Tranditionally, the `renaming'
of the representative variable is called $\alpha$-conversion and you can
define a relation $=_\alpha$ for it.}. Sometimes we may encounter a term
like $(\lambda x\ (x\ (\lambda x\ (x\ x))))$. Since $x$ is not free in the
inner term $(\lambda x\ (x\ x))$. We can change it to $y$, i.e.,
$(\lambda y\ (y\ (\lambda x\ (x\ x))))$, and the above equality of 
$\lambda$-terms says they are the same. Thus the equality means the
convention that when there's a naming conflict, the inner bounded 
variable will {\it shadow} the outer one. Now we can define the 
substitution process.


\begin{definition}(substitution of $\lambda$-terms)
    For $M,N\in\Lambda$ and variable $x$, the substitution of $N$ for
    $x$ in $M$, written as $M[x:=N]$ is recursively defined as follows.
    \eq{
        x[x:=N] & = N \\
        y[x:=N] & = y & y\neq x\\
        (P\ Q)[x:=N] & = (P[x:=N]\ Q[x:=N]) \\
        (\lambda y\ P)[x:=N] &= \lambda y.P[x:=N] & x\neq y,y\notin\FV(N)\\
    }
\end{definition}


\begin{definition}\label{beta-reduction}($\beta$-reduction)
    We define a relation $\to_\beta$ between $\lambda$-terms
    $$(\lambda x.M)\ N \to_\beta M[x:=N],$$ and extend it to be
    the closure under the rules:
    $$
    \begin{array}{lcl}
        P\to_\beta Q & \Rightarrow & \forall x\in V,\lambda x.P\to_\beta\lambda x.Q \\
        P\to_\beta Q & \Rightarrow & \forall R\in \Lambda,P\ R\to_\beta Q\ R \\
        P\to_\beta Q & \Rightarrow & \forall R\in \Lambda,R\ P\to_\beta R\ Q\\
    \end{array}
    $$
\end{definition}

For example, $((\lambda x\ (x+x))\ 2)\to_\beta(2+2) = 4$. 
A step of $\beta$-reduction is a minimal (atomic) step of
`computation'. It means how to perform a computation when an abstraction
is applied to another term. In \cite{homotopy-type-theory}, this is the 
computation rule for function type. We also define the transitive and 
reflexive closure $\twoheadrightarrow_\beta$ and the equivalent closure 
$=_\beta$.

\begin{definition}
    (multi-step $\beta$-reduction and $\beta$-equality)
    The multi-step $\beta$-reduction $\twoheadrightarrow_\beta$ is
    generated under the rules:
    $$
    \begin{array}{lcl}
        P\to_\beta Q & \Rightarrow & P\twoheadrightarrow_\beta Q \\
        P\twoheadrightarrow_\beta Q, Q\twoheadrightarrow_\beta R & \Rightarrow & P\twoheadrightarrow_\beta R \\
        P\twoheadrightarrow_\beta P &&
    \end{array}
    $$
    and the relation $=_\beta$ is generated by
    $$
    \begin{array}{lcl}
        P\to_\beta Q & \Rightarrow & P=_\beta Q \\
        P=_\beta Q, Q=_\beta R & \Rightarrow & P=_\beta R \\
        P=_\beta P && \\
        P=_\beta Q &\Rightarrow& Q=_\beta P
    \end{array}
    $$
\end{definition}

For the real world, you can introduce numbers and basic arithematics like
the example $\lambda x. x \cdot 2 + 1$, but for a pure theoretical 
purpose, you can define the natural numbers as $c_n=\lambda s.\lambda z. s^n\ z$, 
where $s^n\ z = s\ (s\ (\cdots\ (s\ z)))$. Those $c_n$ are called {\it
Church numerals}. Try to prove the term $$A_+=\lambda x.\lambda y.
\lambda s.\lambda z.x\ s\ (y\ s\ z)$$ satisfies the relation 
$A_+\ c_n\ c_m=_\beta c_{m+n}$. You may want to consider what are the 
multiplication and exponent. 

The last example is only available in untyped lambda calculus, the fixed 
point combinator. Compared with the traditional 
\newcommand{\Ycomb}{\mathbf{Y}}
$$\Ycomb = \lambda f.(\lambda x.f\ (x\ x))\ \lambda x.f\ (x\ x)$$
such that for any $\lambda$-term $F$, 
$F\ (\Ycomb\ F)=_\beta \Ycomb\ F$, the exercise in chapter 1 of 
\cite{Curry-Howard} is more interesting. Let
$$
\begin{array}{lll}
    ? &=& \lambda abcdefghijklmnopqstuvwxyzr.r\ (thisisafixedpointcombinator) \\
    \$ &=& ??????????????????????????
\end{array}
$$
Prove that $\$\ F=_\beta F\ (\$\ F)$ for all $F\in\Lambda$.

Then let's talk about typed lambda calculus. All the terms we defined
above can be applied to any other term, but for a function 
$f: \mathbb{N}\to\mathbb{N}$, you won't apply it to $\sqrt 2$ and 
you won't expect $f(3)$ is an element of the set $\{cat,dog,duck\}$.
Thus we attach each $\lambda$-term with such an annotation. 

Let $U$ be a set of all {\it type variables}. Define the set 
$\Delta$\footnote{In \cite{Curry-Howard}, the letter for such a set is
$\Pi$, but this is used for dependent function type ($\Pi$-type) 
(\autoref{dependent-function}) in HoTT, so I change the notation.} of
types as
$$
    \Delta ::= U \mid \Delta \to \Delta
$$
We want to annotate each $\lambda$-term with a type in $\Delta$. 
For example, $\lambda x. x: a\to a$ for some $a\in U$. Formally speaking, 
the notation `:' is introduced as follows. 

\begin{definition} (Typability)

A context is a set $\Gamma = \{x_1: \tau_1, x_2:\tau_2, \cdots\}$ 
with $x_i\in V$ and $\tau_i\in U$. We define the {\it typability}
relation $\proves$ on the set of all triples $(\Gamma, x, \tau)$, where 
$x\in\Lambda$ and $\tau\in\Delta$ (written as $\Gamma\proves x:\tau$).

\begin{enumerate}
    \item $$\infer{\Gamma, x:\tau\proves x:\tau}{}$$
    This means without any premise (the blank on the line), you
    are allowed to define $\proves$ for triple 
    $(\Gamma\cup\{x:\tau\}, x, \tau)$
    ($\Gamma,x:\tau$ means $\Gamma\cup\{x:\tau\}$).
    \item $$\infer{\Gamma\proves\lambda x.M:\sigma\to\tau}{\Gamma,x:\sigma\proves M:\tau}$$
    This means if you have defined the relation $\Gamma,x:\sigma\proves M:\tau$,
    then you can obtain a new relation $\Gamma\proves\lambda x.M:\sigma\to\tau$.
    (introduction rule for $\to$)
    \item $$\infer{\Gamma\proves M\ N:\tau}{\Gamma\proves M:\sigma\to\tau & \Gamma\proves N:\sigma}$$
    This means you can eliminate the arrow $\to$ in the type by application.
\end{enumerate}

\end{definition}

Note we start with the context $\Gamma$, where only variables are annotated
by types. Our target is to define how the $\lambda$-terms are annotated.
Thus a suitable target is to introduce all $\to$ until $\Gamma$ is empty
(rule 2). For example
\begin{enumerate}
    \item $\proves \lambda x.x: \sigma\to\sigma$
    \item $\proves \lambda x.\lambda y.x: \sigma\to\tau\to\sigma$ \\
    (according to currying, $\sigma\to\tau\to\sigma$ should be 
    $\sigma\to(\tau\to\sigma)$)
    \item $\proves \lambda f.\lambda g.\lambda a. f\ a\ (g\ a):
    (A\to(B\to C))\to(A\to B)\to(A\to C)$
\end{enumerate}

Coincidently, the context can be given another interpretation. The 
{\it range} of a context $\Gamma$ is defined by
$|\Gamma|=\{\tau\in\Delta\mid(x:\tau)\in\Gamma\}$. The type set $\Delta$
can also be understood as the set of all well formed formulas (without
conjunction or disjunction) in propositional logic. Then you can easily 
observe the following theorem.
\begin{theorem}
    (Curry-Howard Correspondence).
    \begin{enumerate}
        \item If $\Gamma$ is a context and $\Gamma\proves M:\phi$, then
        $|\Gamma| \proves \phi$ in (intuitionistic) propositional logic
        \footnote{See \autoref{intuitionistic-logic}}.
        \item If $\Gamma\proves \phi$ in IPC 
        ({\bf I}ntuitionistic {\bf P}ropositional {\bf C}alculus), then
        you can define a context $\Gamma'=\{(x_\theta):\theta\mid\theta\in\Gamma\}$
        and find an $M: \phi$ such that $\Gamma'\proves M:\phi$. 
    \end{enumerate}
\end{theorem}

The previous discussion is mainly for application of functions, 
i.e. the {\it implicational fragment} of propositional logic.
Actually we only defined rules for how a `function type' $\sigma\to\tau$
is constructed by some $\lambda$-term and how to compute it by application.

We adopt the same style to define types for conjunction and disjunction.
The type set $\Delta$ is extended to
$$
\Delta ::= V\mid\Delta\to\Delta\mid\Delta\times\Delta\mid\Delta+\Delta
$$
with following annotating rules:
$$
\infer{\Gamma\proves (M, N): \sigma\times\tau}{\Gamma\proves M:\sigma, N:\tau}
\ \ \ 
\infer{\Gamma\proves \pi_1(L):\sigma}{\Gamma\proves L: \sigma\times\tau}
\ \ \ 
\infer{\Gamma\proves \pi_2(L):\tau}{\Gamma\proves L: \sigma\times\tau}
$$
$$
\infer{\Gamma\proves \left<1, M\right>:\sigma + \tau}{\Gamma\proves M:\sigma}
\ \ \ 
\infer{\Gamma\proves \left<2, N\right>:\sigma + \tau}{\Gamma\proves N:\tau}
$$
$$
\infer{\Gamma\proves \text{\bf case}(L;x.M;y.N): \rho}{
    \Gamma\proves L:\sigma+\tau &
    \Gamma, x:\sigma\proves M:\rho &
    \Gamma, y:\tau\proves N:\rho
}
$$
where 
$$
\begin{array}{c}
    \pi_1((M_1, M2)) \to_\beta M_1 \\
    \pi_2((M_1, M2)) \to_\beta M_2 \\
    \\
    \text{\bf case}(\left<1, N\right>;x.K;y.L)\to_\beta K[x:=N] \\ 
    \text{\bf case}(\left<2, N\right>;x.K;y.L)\to_\beta L[y:=N].
\end{array}
$$

Of course, you may want to extend the $\lambda$-term to have such
syntactic definitions, but actually they can be expressed by lambda
calculus directly \cite{Curry-Howard} (in an inconvenient way). 
I prefer to define them syntactically instead of using lambda terms.
(The technique used to define those $\lambda$-terms is called {\it CPS 
transformation}, which is another interesting theoretical thing.)

\begin{caveat}
    \label{constructor-destructor}
    The notations $\pi_i$ and $\text{\bf case}$ are defined syntactically
    just like the lambda abstraction and application. They can only be
    introduced or eliminated according to the rules above. You cannot
    understand them like tuples in set theory. Instead, a better
    understanding is that you actually defined such constructors 
    $(,)_{\sigma\times\tau}: \sigma\to\tau\to\sigma\times\tau$ (the comma
    in the tuple) and $\name{inj}^1_{\sigma+\tau}: \sigma\to\sigma+\tau$, 
    $\name{inj}^2_{\sigma+\tau}: \tau\to\sigma+\tau$ like you are allowed
    to write the lambda abstraction. Similarly, the application $(M\ N)$
    never requires that $M$ should be an abstraction, you just think it 
    as an `applicative' term (e.g, with type $\sigma\to\tau$). You don't 
    know whether it has the form $(\lambda x.\cdots)$. It's purely a
    syntactic symbol. The type $\sigma\times\tau$ is also a symbol. We
    do not assume any structure like $(M, N)\in M\times N$. To make
    use of it, we define the destructor $\pi_1:\sigma\times\tau\to\sigma$
    and $\pi_2:\sigma\times\tau\to\tau$ and the computation rule. That is
    when you apply a constructor to a destructor, what should happen.
    Like the application $(M\ N)$, if $M$ is an abstraction, then we can 
    perform the $\beta$-reduction. This will be called {\it induction 
    rule} later with a more general description. That is to define
    such a rule, we only need to define them on all constructors.
    When a destructor is applied to a constructor, we define what is a
    reduction; otherwise, they are pure symbols (like $(M\ N)$).
    (You may want to think of them as axioms. The powerset axiom allows 
    you to define a set from another and others define how to use it.)
\end{caveat}

The above is the counterpart of propositional logic, which we often
refer to as {\it simply typed $\lambda$-calculus}. 

\section{The Usage of Meta Language}
Though we have two things, $\lambda$-terms and types, as in set theory,
we will call them both `type'. A term must be introduced with its type 
and the type itself can be considered as the element in a higher order 
type universe. However the relation `$:$', unlike `$\in$', is not a 
propositional relation. This relation is judged in our meta-language. 
(Think about the rules above. The `$:$' relation is not derived from a 
rule, but a syntactic constraint.) According to the Curry-Howard 
Correspondence, a proposition $A$ is provable if and only if a term 
$a$ of this type is constructable ($A$ is inhabited by $a$). Thus the 
relation $a:A$ is precisely the relation $\proves A$ in our meta language. 

Another thing is the equality. In first order logic, we have a distinct
symbol `$\doteq$' for the language, but it is interpreted as the identity
in the model, i.e. the meta-language. In type theory, they are called
propositional equality and judgemental equality. We may prove `$=$' as
a proposition (we can find a term of this type), while the 
$\alpha$-conversion (renaming of a $\lambda$-term) shows another equality,
the judgemental equality, an equality prior to the existence of type
theory. This judgement or definitional equality is denoted by `$\equiv$',
or sometimes to emphasize it's a definition, I will use `$\defn$'. In 
the rest of this chapter and \autoref{HoTT}, `$=$' will always mean
propositional equality as a type, and `$\equiv$' is the normal equality
we use for mathematics (judgemental equality). In other chapters, I will 
still use `$=$'. 

Now the only things we have to handle are the relation `$:$' and the 
judgemental equality $\equiv$, and to define the {\it function type} 
`$\to$', we first define the syntax for the type set, i.e., if $A$ and 
$B$ are types, then so is $A\to B$. This is called the {\bf formation 
rule} for $\to$. Then we have to show when this type is {\it inhabited}, 
i.e., to define an element $f$ of this type $f:A\to B$, I have to use 
$\lambda$-abstraction. This extends the syntax of $\lambda$-term. 
Such a syntax is called the {\bf constructor} of a new type 
(Another example is the comma $(,): A\to B\to A\times B$. Though I write 
it as a function, it is not defined as a function, but a new syntax. 
After introducing the universe, you can express it as a function. 
The formation rule is like you can always form a new formula but you
have to first give how to prove such a new formula by constructors and
only use constructors to `prove' this formula. The abstraction can never 
be described by a simpler term. Actually you can even introduce more 
syntax like the abstraction, e.g. the $\lambda\mu$-calculus 
\cite{lambda-mu-calculus}.) We also define the {\bf computation rule}
($\to_\beta$) $(\lambda x. M)\ N \defn M[x:=M]$ showing the 
{\bf elimination rule} for the function type. And we define the 
$\alpha$-conversion to identify two $\lambda$-terms, which is called 
the {\bf uniqueness rule}.

\section{The Universe of Type}
\label{type-universe}
I've only defined the syntax for types so far, but you may want to
ask, what is a type or what is the `universe' for types, just like the 
`set' of all sets? As in set theory, if the universe includes itself then
we will encounter Russell's paradox \cite{the-paradox-of-trees-in-type-theory}.
Instead, we define a hierarchical universe.
$$
    \universe_0: \universe_1: \universe_2: \cdots.
$$
This means each universe can be referred to in the language under a higher
level universe. We also require that if $A: \universe_i$ then 
$A: \universe_{i+1}$, but it has the disadvantage that an element won't
have a unique type and if you `proved' a theorem in $\universe_i$,
then you have to prove it again in universe $\universe_{i+1}$ because
you have more elements though the proofs are the same\footnote{In Idris
and Agda, the universe can be written in a `polymorphic' way, i.e. they
use dependent function type (\autoref{dependent-function}) to avoid this problem, maybe
this is a good solution in engineering. I'm not sure whether this will break
the consistency of the logic, but this inconvenience won't be a problem
in our development of type theory}. 

One would like to omit the subscription $i$ thus ignoring the 
inconvenience to some extent. For example, $A:\universe$ should have been 
some $A:\universe_i$ and $\universe:\universe$ is 
$\universe_i:\universe_{i+1}$, but be carefule that 
$\lambda(i:\nat). \universe_i$ is not a well defined term (The type 
$\nat$ will be introduced later, and $U_i$ is also understand as term.
There's no difference between a type and an element of that type. They
are all called {\it type}\footnote{In contrast to the traditional
programming language where {\it type} and {\it data} are different concepts}).

We need the universe $\universe$ to index a family of types, e.g. 
$B:A\to\universe$. A typical case is that when this $B$ is a `constant'
function. Let me also write that constant as $B:\universe$. Thus we can define
the constant function $(\lambda(x:A).B):A\to \universe$.


\section{Dependent Function Type}
The type family, as you may guess, is used to define some {\it polymorphic}
functions. For example a function of type $(A\to(B\to C))\to(A\to B)\to
(A\to C)$ can be define for every type $A, B, C$. You may expect such
a function with quantifier: $\forall (A\ B\ C: \universe), 
(A\to(B\to C))\to(A\to B)\to (A\to C)$, and define the term
$\lambda (A:\universe).\lambda (B:\universe).\lambda (C:\universe).\lambda (f: A\to B\to C).
\lambda (g: A\to B).\lambda (a: A). f\ a (g\ a): \forall (A\ B\ C:\universe), 
(A\to(B\to C))\to(A\to B)\to (A\to C)$.

Look at the term I defined above, it's too long to be readable and apparently, 
this is unnecessary. Once we give the type $\forall (A\ B\ C:\universe), 
(A\to(B\to C))\to(A\to B)\to (A\to C)$, we can infer the type of each 
variable in $\lambda A.\lambda B. \lambda C. \lambda (f: A\to B\to C). 
\lambda (B: A\to B).\lambda a. f\ a\ (g\ a)$ (though I add the redundant
types to $f$ and $g$ as hints). Actually there are two type systems, the
one I introduced is called {\it typing \`a la Curry}, which is a process
giving types to $\lambda$-terms. In contrast, there's another {\it typing
\`a la Church} which forces every variable to be assigned with a type.
The Curry typing is helpful when you want to write less type notations 
while the Church typing is how we write a proposition. There are some
further similarity and difference between these two type systems. For
more details, see Chapter 3 of \cite{Curry-Howard}. Actually I was
cheating because the Curry-Howard Correspondence should have been stated
with typing \`a la Church, but as you've seen, we don't have to be that 
strict with ourselves. Generally I will write the type explicitly, but if 
it can be inferred from the context, why not just omit them? The 
quantifiers also gives us a way to get rid of `free variables', and we 
don't need to be that strict. I will perhaps use subscriptions to 
indicate the bounded variables.
(For example, $f_{ABC}: (A\to B\to C)\to(A\to B)\to(A\to C)$).

In type theory, the `universal quantifiers' are represented by {\it
dependent function type ($\Pi$-types)}.

\begin{definition}
    \label{dependent-function}
    (Dependent function Type) Given a type $A:\universe$ and a family 
    $B:A\to\universe$, we form a new type $\Pi_{(x:A)}B(x): \universe$.

    The constructor for this type is still the lambda abstraction,
    but it allows you to introduce a type as a variable after the $\lambda$
    notation and that can be used in type annotation of a varaible later.

\end{definition}

For example the term $\lambda (A:\universe).\lambda (B:\universe).\lambda (C:\universe).\lambda (f: A\to B\to C).
\lambda (g: A\to B).\lambda (a: A). f\ a (g\ a)$ is such a constructor.
The dependent function type is a generalization of functions, for if
$B:A\to\universe$ is a constant function, the type $\Pi_{(x:A)}B$ is
the ordinary function $A\to B$. Sometimes the $\Pi$-type is called 
dependent function or dependent type. The name $\Pi$ is due to the fact
that dependent function can give a way to define product (exercise in
chapter 1 of \cite{homotopy-type-theory}).

Let me show some other examples.

\newcommand{\swap}{\name{swap}}

$$
    \swap: \prod_{A:\universe}\prod_{B:\universe}\prod_{C:\universe}
    (A\to B\to C)\to (B\to A\to C)
$$
$$
    \swap \defn \lambda A.\lambda B.\lambda C.\lambda g.
    \lambda b.\lambda a. g\ a\ b
$$

It is not wise to write so many $\lambda$s. Sometimes I accept the
traditional manner to define a function, i.e. I define the `{\bf swap}' as
$$
    \swap\ A\ B\ C\ g \defn \lambda b.\lambda a.g\ a\ b.
$$

This is understand as given the type $A,B,C$ and a function $g:A\to B\to C$,
{\bf swap} gives you a new function, which takes two arguments $b,a$ and gives
you the result $g\ a\ b$. Since the space is not `\LaTeX-friendly', I may still
use the notation 
$$
\swap(A,B,C,g)\defn \lambda b.\lambda a.g(a)(b).
$$
Be sure to distinguish this from tuple. (Especially when it comes to $f\ (a, b)$),
the application of a function to a tuple. You may also want to express this
as $(f\ (a, b)),f((a,b)),f(a,b)$ or the curried function $f(a)(b)$).

You can also use subscriptions to define it
$$
    \swap_{A,B,C}(g)(b,a) \defn g(a, b).
$$

The identity function is defined as
\newcommand{\id}{\name{id}}
\eq{
    \id &: \prod_{A:U} A\to A \\
    \id_A(x) &\defn x
}

\newcommand{\Id}{\name{Id}}
(This is the identity function, while later the type $\Id_A(a, b)$ 
will mean the equality of two elements $a,b:A$). 

\section{Inductively Defined Natural Numbers}
This time, we define a type with multiple constructors and induction.
Actually there should be some extra requirements\footnote{Inductive types
are initial algebras. In programming language Haskell, they are called
Algebraic data type} for induction because you may easily define the 
Rousell's paradox or some similar one (See chapter 5 of 
\cite{homotopy-type-theory}). 

The natural numbers $\nat$ is defined as a brand new type with
constructors $0: \nat$ and a successor function \newcommand{\succn}{\name{succ}} 
$\name{succ}:\nat\to\nat$. We also adopt the decimal notation 
$1 \defn \succn(0), 2\defn\succn(1)$. Though I write the $\succn$ like a
function, it behaves a bit different from a normal function. For example,
if $\succn(a) = \succn(b)$, we may then assume $a = b$ (equality is introduced
later), and we have to show how to `make use of' an element $n:\nat$.
To construct a function $f:\nat\to C$, I have to define what happens for
constructor $0: \nat$ and when $f(n)$ is defined, how to define $f(\succn(n))$.
Let $c_0:C$ and $c_s:\nat\to C\to C$ be two functions, we actually want
the (computation) rules

\eq{
    f(0) & \defn c_0 \\
    f(\succn(n)) &\defn c_s(n,f(n)).
}

In computability theory (recursion theory), this way to define a function
is called {\it primitive recursion}. 

\newcommand{\rec}{\name{rec}}

Though I give the universe of types, I still want to handle this pure 
syntactically. Instead of thinking $\nat$ consists of only $0$ and $\succn$,
this definition rule is written as the following recursion rule (the axioms
are symbolic).
$$
    \rec_\nat:\prod_{C: U}C\to(\nat\to C\to C)\to\nat\to C
$$
with computation rules
\eq{
    \rec_\nat(C, c_0,c_s,0) &\defn c_0 \\
    \rec_\nat(C, c_0,c_s,succ(n)) &\defn c_s(n, rec_\nat(C,c_0,c_s,n))
}

\newcommand{\add}{\name{add}}
For example we can define the sum of two natural numbers;
$$
    \add: \nat\to\nat\to\nat
$$
$$
    \add \defn \rec_\nat(\nat\to\nat,\id_\nat,\lambda n.\lambda g.\lambda m.\succn(g(m)))
$$

This is, $\add$ is a function from $\nat$ to $\nat\to\nat$ (when 
$C$ is $\nat\to\nat$). To define such a function, we first define 
$\add\ 0$, which is apparently the identity function 
($0+n \defn n$), and if $\add\ n$ is defined, we define 
$\add\ (\succn\ n)$ to be the function which takes a number, applies
the previous $\add\ n$ and takes the successor of the result, i.e.,
a function $c_s(n:\nat, g:\nat\to\nat) \defn (\lambda m.\succn(g(m)): 
\nat\to\nat)$, or $\add(\succn(n))\defn \lambda m. \succn(\add(n, m))$, 
i.e., $\add(\succn(n), m)\defn\succn(\add(n, m))$. (It's convenient
to write $\add(n, m)$ as $n+m$)

Similarly we have 
$$
    \name{double}: \nat\to\nat
$$
$$
    \name{double} \defn \rec_\nat(\nat, 0, \lambda n.\lambda y.\succn(\succn(y))),
$$
and the predecessor
\newcommand{\pred}{\name{pred}}
$$
    \pred: \nat\to\nat
$$
$$
    \pred \defn \rec_\nat(\nat, 0, \lambda n.\lambda m. n),
$$
where $\pred(0)\defn0$ and $\pred(\succn(n))\defn (\lambda n.\lambda m. n)\ n\defn n$.

In the previous examples, you may notice that it suffices to define the
so called `computation rule' on each constructor. Actually there's a
{\it pattern matching} manner to define a recursive function, e.g., the
$\pred$ can be defined as
\eq {
    \pred(0) &\defn 0 \\
    \pred(\succn(n)) &\defn n
}
and the $\name{add}$ is defined as
\eq {
    0 + m &\defn m \\
    (\succn\ n) + m &\defn \succn\ (n + m).
}

But be sure such a definition can be written with $\rec$, or you may
get some pathological thing, e.g.
\eq{
    \name{loop\_false} &: \nat\to C \\
    \name{loop\_false}(n) &\defn \name{loop\_false}(n)
}
where $C:\universe$ is an arbitrary type. A system where every term
can be derived is inconsistent. Later I will construct a type $\zero$ 
without constructors. When $C$ is $\zero$, $\name{loop\_false}(0):\zero$ 
means absurdity (because no constructor is provided). So far we haven't
defined any `partial functions'. A turing machine may never {\it halt}
and $\name{loop\_false}$ is such a case. (This could be further developed
into a computational version of Godel's incompleteness theory.)

Let's generalize this rule $\rec$ a bit. Suppose now I'm going to define
a dependent function on $\nat$ or predicate something about natural numbers,
i.e. you have the type $C: \nat\to\universe$. This is easily understood as
a predicate and to prove something about natural numbers, we can use inductions:
$$
    \infer{\Pi_{n:\nat}C(n)}{C(0) & \Pi_{n:\nat}(C(n)\to C(\succn(n)))}
$$
\newcommand{\ind}{\name{ind}}
Thus we have an induction rule 
$$
\ind_\nat: \prod_{C:\nat\to\universe}C(0)
    \to(\Pi_{n:\nat}C(n)\to C(\succn(n)))
    \to\Pi_{n:\nat} C(n).
$$

This is a generalized computation or elimination rule because when
$C$ is a constant type, it is just the recursion rule $\rec_\nat$. 
Thus we also have the following computation rule.
\eq{
    \ind_\nat(C,c_0,c_s,0) &\defn c_0 \\
    \ind_\nat(C,c_0,c_s,\succn(n)) &\defn c_s(n,\ind_\nat(C,c_0,c_s,n))
}
From our definition, this should be the only way to make use of a type.
See chapter 5 of \cite{homotopy-type-theory}. 

\newcommand{\refl}{\name{refl}}
Let me show you how to make use of this rule. There's a obvious `proposition'
$\forall n:\nat, n + 0 = n$. (Note from the computation rule, we only know
$0+n=n$.) The $=$ type is not introduced yet but its constructor is quite 
simple. Given a type $A:\universe$ and $x,y:A$, you can {\it form} a new 
type $x=_Ay$ while the only constructor for this type is $\refl_x: x=_Ax$. 
(Note that syntactically the {\it formation rule} always allows you to 
give a new type while the relation `$:$' is defined only by constructors.)

To prove that proposition, I expect the type $\Pi_{(n:\nat)}n+0 =_\nat n$
to be inhabited. According to the induction rule, I choose $C: \nat\to\universe$
to be $C(n)\defn n+0=_\nat n$. 
Now, I only need to `prove' $C(0)$ and $\Pi_{n:\nat}C(n)\to C(\succn(n))$.

$$
    C(0) \defn 0 + 0 =_\nat 0
$$
According to the computation rule, we know $0 + 0\defn 0$. Thus
$\refl_0: C(0)$ will be the witness. Now for the `induction step',
I reform the target as
$$
\infer{\succn(n) + 0 =_\nat \succn(n)}{n:\nat & n+0=_\nat n}
$$
Again by the computation rule, 
$\succn(n) + 0 =_\nat \succn(n)\defn \succn(n+0)=_\nat\succn(n)$. 
Now let's admit the leibniz rules. (though I am reluctant to do this, 
but it seems that without equality I could hardly prove anything. 
This will be introduced later.)

\begin{definition}
    \label{leibniz}
    $$
    \name{leibniz}: \prod_{A,B:\universe}\prod_{f:A\to B}\prod_{x,y:A}
        x=_Ay\to f(x)=_Bf(y)
    $$ 
\end{definition}
Clearly the type $\Pi_{n:\nat}C(n)\to C(\succn(n))$ can be inhabited by
$$
    \lambda (n:\nat).\lambda (eq:n+0=_\nat n). \name{leibniz}
    (\nat,\nat,\succn,n+0,n,eq). 
$$
This example also shows the difference between `judgemental equality' 
and `propositional equality'. The symbol $n+0$ will never equal $n$
judgementally, but they can be proved equal. This shows that the equality
is not only obtained by reflexivity, which further raises the homotopy
interpretation in \autoref{homotopy}.

\section{Some Simple Types}
Now that we have seen how a complicated type is defined (actually we
extend the typed lambda calculus), let me introduce some simpler types.
\newcommand{\inl}{\name{inl}}
\newcommand{\inr}{\name{inr}}
\begin{enumerate}
    \item The {\it product type} is defined with constructor {\it comma}
    $(,)_{A,B}:A\to B\to A\times B$. Actually if we have $a: A,b: B$, then
    we write $(a, b): A\times B$ omitting a lot of extra syntactic requirements.
    The {\it recursor} and {\it induction principle} are
    $$
    \rec_{A\times B}: \prod_{C:\universe}(A\to B\to C)\to A\times B\to C \\
    $$
    $$
    \rec_{A\times B}(C,g,(a,b)) \defn g(a)(b)
    $$
    $$
    \ind_{A\times B}:\prod_{C:A\times B\to\universe}
        (\prod_{x:A}\prod_{y:B}C((x,y)))\to\prod_{x:A\times B}C(x)
    $$
    $$
        \ind_{A\times B}(C,g(a,b))\defn g(a)(b)
    $$
    Here I defined the recursor and induction rules, instead of the
    equivalent $\pi_1,\pi_2$. They are defined as 
    $\pi_1\defn\rec_{A\times B}(A,\lambda a.\lambda b. a)$
    $\pi_2\defn\rec_{A\times B}(A,\lambda a.\lambda b. b)$.
    \item The unit type $\one$ is defined with only one constructor without
    any extra parameters ($\star:\one$). The recursor and induction principle
    are 
    $$
        \rec_\one:\prod_{C:\universe}C\to\one\to C
    $$
    $$ 
        \rec_\one(C,c,\star) \defn c
    $$
    $$
        \ind_\one:\prod_{C:\one\to\universe}C(\ast)\to\pi_{x:\one}C(x)
    $$
    $$
        \ind_\one(C,c,\ast) \defn c
    $$

    \item Given $A, B:U$, we form the {\it coproduct type} $A+B$ with
    constructors $\inl:A\to A+B$, $\inr:B\to A+B$ (`left injection' and
    `right injection'). The induction principle is
    $$
        \ind_{A+B}: \prod_{C:(A+B)\to\universe}
            (\prod_{a:A}C(\inl(a))) \to
            (\prod_{b:B}C(\inr(b))) \to
            \prod_{x:A+B} C(x)
    $$
    $$
        \ind_{A+B}(C,g_0,g_1,\inl(a))\defn g_0(a)
    $$
    $$
        \ind_{A+B}(C,g_0,g_1,\inr(a))\defn g_1(a)
    $$
    \item The type $\one$ is the nullary-product (terminal object). We 
    can similarly define the nullary-coproduct $\zero:\universe$. There's
    no way to construct the inhabitant of such a type, and we can easily
    figure out the induction principle 
    $\ind_\zero: \prod_{C:\zero\to\universe}\prod_{z:\zero}C(z)$. The
    simpler version $\rec_\zero:\prod_{C:\universe}\zero\to C$ implies
    that if $\zero$ had been inhabited, then any type, including itself, 
    would have been inhabited, i.e., any `proposition' would have been 
    provable. We then consider $\zero$ as absurdity $\bot$ in intuitionistic
    logic and I can give the counterpart of negation in Curry-Howard 
    Correspondence that $\neg\ P$ is understood as a function $P\to\zero$.
    To prove something is wrong, you prove that if this fact is true, 
    then you can derive absurdity and thus everything. The recursor 
    $\rec_\zero$ is the principle {\it ex falso (sequitur) quodlibet}.

    \item The {\it boolean} type $\two:\universe$ is a type with exactly
    two constructors $0_\two,1_\two:\two$. The induction principle is 
    $$
    \ind_\two:\prod_{C:\two\to\universe}C(0_\two)\to C(1_\two)\to\prod_{x:\two}C(x)
    $$
    $$
        \ind_2(C,c_0,c_1,0_\two)\defn c_0
    $$
    $$
        \ind_2(C,c_0,c_1,1_\two)\defn c_1
    $$
    Note this is not the boolean value of our language, but as a target.
    In first order logic, we define the {\it correctness} of a sentence
    by interpreting it to a boolean value (or more generally, a boolean
    algebra (Chapter 2 of \cite{Curry-Howard})), but this is not the
    (judgemental) correctness in our meta-language (like $\equiv,:$).
    For example, we can define a relation $\name{leb}:\nat\to\nat\to\two$ 
    (`less than or equal to in boolean') by pattern matching. (Please try
    rewriting it with the recursor $\rec_\nat$.)
    \eq {
        \name{leb}(0, m) & \defn 1_\two \\
        \name{leb}(\succn(n), 0) & \defn 0_\two \\
        \name{leb}(\succn(n),\succn(m)) & \defn \name{leb}(n,m)
    }
    However, we can also define this relation as a new type.
    Define $\name{le}: \nat\to\nat\to\universe$ (like the $\times:\universe
    \to\universe\to\universe$ for product type can also be understood as a 
    `function') with constructors $\name{le}_n(n): \name{le}(n, n)$ and
    $\name{le}_S(n:\nat, m:\nat, H: \name{le}(n, m)): \name{le}(n,\succn(m))$.
    (This is a dependent version of constructors. If you are not happy 
    with it, look at the next section.)
    You can try to prove\footnote{You can find this in the first volume of
    \cite{SF}. I strongly recommend you not prove it with your pen but with
    the help of a computer proof assistant.} 
    $\Pi_{n,m:\nat}\name{le}(n,m)\to \name{leb}(n,m)=_\two 1_\two$.
\end{enumerate}

\section{Dependent Pair}

Given type $A:\universe$, and a family $B:A\to\universe$, I want a relation
between some $a: A$ and $p: B(x)$. Trivially, let me just compose them
together with a tuple $(a, p)$. We call such a behavior {\it dependent 
pair} or $\Sigma$-type written as $(a,p):\Sigma_{x:A}B(x)$ because this 
looks like a product type while the type of the second element in the
tuple {\it depends} on the first element. Of course, the constructor
is $(,)_{A,B:A\to\universe}: \Pi_{x:A}B(x)\to\Sigma_{y:A}B(y)$.
When $B$ is constant, $\Sigma_{x:A}B\defn(A\times B)$. Similarly we can
define $\pi_1:\Sigma_{x:A}B(x)\to A$ and 
$\pi_2:\Pi_{(p: \Sigma_{x:A}B(x))}B(\pi_1(p))$. Such a definition coincides
with the requirement for {\it existential quantifier} ($\exists$) in
intuitionistic logic, i.e. if something exists, you must construct it.
You can further make a group or at least a 
$\name{Magma}\defn \Sigma_{A:\universe}(A\to A\to A)$ with it.
The induction principle is
$$
    \ind_{\Sigma_{x:A}B(x)}:\prod_{C:(\Sigma_{x:A}B(x))\to\universe}
    (\Pi_{a:A}\Pi_{b:B(a)}C((a, b)))\to
    \prod_{p: \Sigma_{x:A}B(x)} C(p)
$$
$$
    \ind_{\Sigma_{x:A}B(x)}(C,g,(a,b)) \defn g(a)(b).
$$

The name $\Sigma$ is related to the disjoint unions. For example,
when $A\equiv \two$, we can define 
$A+B\defn \Sigma_{x:\two}\rec_\two(\universe,A,B,x)$ with 
$\inl(a)\defn(0_\two,a)$ and $\inr(b)\defn(1_\two,b)$. I'm not sure
whether this is true or not, but it seems that we can treat $\Pi$-types
and $\Sigma$-types as products and coproducts of indexed families of 
types. I haven't found any material about how to categorize dependent
types but as you may observe, there's a category of simply typed lambda
calculus. 

\section{Categroy of Simply Typed Lambda Calculus}
We use the definitional or judgemental equality $=_\beta$ as an
equivalence relation on the set of all $\lambda$-terms and make it
into a category:

\begin{itemize}[label={}]
    \item objects: the types
    \item arrows: If $c: A\to B$ is a $\lambda$-term without {\it free
    variables} (so-called closed term), then $[c]$ is an arrow from
    $A\to B$.
    \item Identities: $1_A$ is $[\lambda x: A.x]$.
    \item Compositions: If $[b]: A \to B$, $[c]: B\to C$, then
    $[c]\circ [b] = [\lambda x. c(b(x))]$.
\end{itemize}

\begin{remark}
    The equivalence relation defines a rule to `identity' different
    proofs. For example\footnote{This example comes from \cite{Curry-Howard}}
    $$
        \infer{\proves\lambda x:\phi.\lambda y:\phi.x:\phi\to\phi\to\phi}
        {
            \infer{x:\phi\proves\lambda y:\phi.x:\phi\to\phi}{
                x:\phi,y:\phi\proves x:\phi
            }
        }
    $$
    and
    $$
        \infer{\proves\lambda x:\phi.\lambda y:\phi.y:\phi\to\phi\to\phi}
        {
            \infer{x:\phi\proves\lambda y:\phi.y:\phi\to\phi}{
                x:\phi,y:\phi\proves y:\phi
            }
        }
    $$
    are two different proofs ($\lambda x. \lambda y. x\neq_\beta
    \lambda x.\lambda y. y$) of
    $$
        \infer{\proves\phi\to\phi\to\phi}{
            \infer{\phi\proves\phi\to\phi}{\phi\proves\phi}.
        }
    $$
\end{remark}

Such a category is a so-called CCC
({\it {\bf C}atesian {\bf C}losed {\bf C}ategory}, a category
with all finite products and exponentials).
The products and exponentials are obvious. Tranditionally, we would
like to extend the syntax of types with two more constants $\top$($\one$)
and $\bot$($\zero$). Thus we have terminals and initials in this category. 

Given a $\lambda$-calculus $\mathcal{L}$ (different variable sets)
and a CCC $\mathbf{C}$, we define a {\it model} of $\mathcal{L}$ in
$\mathbf{C}$ as an assigement
\newcommand{\assign}[1]{\llbracket#1\rrbracket}
\eq {
    X\ \text{type} &\mapsto \assign{X}\ \text{object} \\
    b: A\to B\ \text{term} &\mapsto \assign{b}: \assign{A}\to\assign{B}\ \text{arrow}
}
such that
$$
\mathcal{L}\proves [a]=_\beta[b]: A\to B\text{ implies }
\assign{a}=\assign{b}: \assign{A}\to\assign{B}
$$

In \cite{cat-awodey}, the author mentioned that since topoi are
CCCs, it's natural to describe them with type theory. Combining
this with the first-order language, the subobject classifier provides
a natural interpretatioon of higher-order logic by exponentials of
the classifier. This part is vaguely described and is picked from the
book \cite{Introduction-to-higher-order-categorical-logic}. I think 
later I will dive into that book to fully understand this. 
