\chapter{Other Useful Examples}
\label{monad-examples}

\newcommand{\Mon}{\cat{Mon}}

\section{Free Monoid is a Monad}
We know the free construction is an adjunction \cite{simmons2011introduction}.
(Please distinguish the monad as a `{\it monoid in the category of
endofunctors}' from a concrete monoid.)
Now let's inspect how it makes a monad. 

Let $\Mon$ be the category of all monoids. For a set $A$ in $\Sets$, the 
free monoid $FA$ generated by $A$ is a monoid of all words with alphabet 
$A$ whose binary multiplication is concatenation of words. We apparently 
have the natural inclusion $\eta_A:A\to |FA|$, where 
$|\cdot|:\Mon\to\Sets$ is the forgetful functor (the underlying set).
The free problem of it is solved as follows. 

\begin{definition}
    Let $\Src\overset{G}{\longleftarrow}\Trg$ be a functor, and suppose
    we have object assignemnt $A\to FA$ (not a functor) and morphism 
    assignment $\eta_A: A\to (G\circ F)A$ (not a natural 
    transformation)\footnote{Though they are a functor and a natural transformation,
    this is not a requirement.}.
    A $G$-free solution is a lift $(\cdot)^\sharp: \Src[A,GS]\to
    \Trg[FA, S]$ such that for each $f:A\to GS$ in $\Src$, there exists
    a unique morphism $f^\sharp:FA\to S$ making the following diagram
    commute.
    $$
    \begin{tikzcd}
        A \arrow{r}{f} \arrow[swap]{d}{\eta_A} & GS \\
        GFA \arrow[swap]{ru}{G(f^\sharp)} &
    \end{tikzcd}
    $$
\end{definition}

In the case of monoid, this is known as the universal property of free
construction, i.e. for each $f:A\to M$, there exists a unique 
$f^\sharp:FA\to M$ such that $|f^\sharp|\circ \eta_A=|f|$. 
Clearly the two Hom-sets are isomorphic. It can be further shown that
free construction is an adjunction. Let's consider how it raises a
monad. Let $T=GF$. The $\eta$ is the unit transformation. For any 
$f:A\to G(FB)$, we have a unique morphism $f^\sharp: FA\to FB$ and
we can apply $G$ to get $G(f^\sharp): TA\to TB$. It's not hard to 
verify that $f^\ast=G(f^\sharp)$ gives the Kleisli triple.
For a free monoid $FA$ generated by $A$, the $\bind$ behavior (or
the lift $(\cdot)^\ast$) says that if you have $f:A\to |FB|$, then
you can define $f^\ast:|FA|\to |FB|$ such that $f^\ast(a_1a_2\cdots a_n)
=f(a_1)f(a_2)\cdots f(a_n)$, where $a_1\cdots a_n$ is a word in the
free monoid $FA$. Note $f(a_1)f(a_2)\cdots f(a_n)$ is also a word
concatenation. This behavior is a bit like the union of the monad 
$\powerset$.

\newcommand{\listtype}{\name{list}}
\newcommand{\nil}{\name{nil}}
\newcommand{\cons}{\name{cons}}

In computer science, free monoid generated by $A$ is often called a 
{\it list} of type $A$. Recusively, we can define a type $\listtype(A)$ 
(just like $A\times B$) with constructors $\nil_A: \listtype(A)$ and 
$\cons:A\to\listtype(A)\to\listtype(A)$. For example 
$\cons(0,\cons(1, \cons(2, \nil)))$ is such a list. Let me denote 
it as $[0,1,2]$ for convenience. In the calculation of
\cref{table-showing-necessity-of-three-axioms}, I made use of this
monad behavior to generate all possible combinations. 

Consdier the following process.
\eq{
    & x \leftarrow [0,1,2,3] \\
    & y \leftarrow [a,b,c] \\
    & [(x, y)]
}
The Kleisli triple will help you generate a list
\eq{
    &[\\
    &\quad (0,a),(0,b),(0,c),\\
    &\quad (1,a),(1,b),(1,c), \\
    &\quad (2,a),(2,b),(2,c), \\
    &\quad (3,a),(3,b),(3,c)\\
    &].
}
This kind of usage is called {\it list comprehension} in computer 
science. 

From this example, it seems that induction is related to free monad. 
I saw a lot of materials about it, where (co)induction and (co)algebra 
are strongly connected. I think I will read about this topic later. 

\section{A Parser is a Monad}

(This section is too engineering related. Feel free to skip it.)

If the generating set is the set of characters, the free monoid is
often called a {\it string} and we denote it with quotation marks.

For example, ``$(0+12)\times2$'' is in the free monoid generated by 
$$A=\{(,),0,1,2,\cdots,9,+,\times\}.$$

\newcommand{\Maybe}{\name{Maybe}}
\newcommand{\Nothing}{\name{Nothing}}
\newcommand{\Just}{\name{Just}}

A parsing problem is to parse the correct value of an element
of $FA$. For example ``$(1+2)\times 3$'' is parsed to $9:\nat$,
but errors should also be handled, e.g. $(+3)2$ is not a sound
formula for us to parse. Thus I generalize this problem to a
function $FA\to \Maybe(\nat)$, where $\Maybe(\nat)$ is a new type with
constructors $\Nothing_\nat:\Maybe(\nat)$ and 
$\Just_\nat: \nat\to \Maybe(\nat)$. This means the elements in $\Maybe(\nat)$
will be $\Nothing$ or those in $\nat$ (written as $\Just(n)$ for some 
$n:\nat$). Clearly the recusor of $\Maybe$ is 
$$
    \rec_\Maybe: \Pi_{A:\universe}\Pi_{B:\universe}
        B\to
        (A\to B)\to 
        \Maybe(A)\to B
$$
such that
$$
    \rec_\Maybe(A,B,b,f,\Nothing) \defn b
$$
$$
    \rec_\Maybe(A,B,b,f,\Just(a)) \defn f(a).
$$

\newcommand{\Parser}{\name{Parser}}

To made this function, let me make it {\it stateful} first. We define
a type alias $\Parser(T)\defn FA\to\Maybe(T\times FA)$, and make a monad
$$
    \unit_T: T\to\Parser(T)
$$
$$
    \unit_T(t)\defn \lambda s.\Just(t, s)
$$
$$
    \bind_{T,Q}: \Parser(T)\to(T\to\Parser(Q))\to\Parser(Q)
$$
\eq{
    \bind_{T,Q}(t,f)\defn & \lambda s. \rec_\Maybe(
        \Maybe(T\times FA), \Maybe(Q\times FA), \\
        &\quad \Nothing, \lambda (a,s').f(a)(s'), t(s)).
}

You may want to rewrite it with pattern match
\eq{
    \bind_{T,Q}(t,f)\defn & \lambda s. \mathbf{case}\ t(s)\ \mathbf{of} \\
        & \Nothing \mapsto \Nothing \\
        & \Just(a,s') \mapsto f(a)(s').
}

\newcommand{\nothing}{\name{nothing}}
For convenience I also define
$$
\nothing_T: \Parser(T)
$$
$$
\nothing \defn \lambda x. \Nothing
$$

Let's look at how it works. We define the following functions.

\newcommand{\get}{\name{get}}
$$
\get: \Parser(FA)
$$
$$
\get \defn \unit(s)
$$

\newcommand{\puts}{\name{put}}
$$
\puts: FA\to\Parser(\one)
$$
$$
\puts(s) \defn \lambda a. \Just(\star, s)
$$

\newcommand{\pitem}{\name{item}}
\eq{
\pitem &: \Parser(A) \\
\pitem & \defn s \leftarrow \get \\
    & \mathbf{case}\ s\ \mathbf{of} \\
    & \quad \nil \mapsto \nothing \\
    & \quad \cons(a,r)\mapsto \bind(\puts(r), \lambda\star.\unit(a))
}

\newcommand{\sat}{\name{sat}}
\eq{
\sat &: (A\to\two)\to\Parser(A) \\
\sat (p) &\defn c \leftarrow \pitem \\
    & \mathbf{case}\ p(c)\ \mathbf{of} \\
    & \quad\one_\two \mapsto \unit(c)\\
    & \quad\zero_\two \mapsto \nothing
}

\newcommand{\pchar}{\name{char}}
\eq{
\pchar &: A\to\Parser(A) \\
\pchar(c) &\defn \sat(\mathbf{is?}\ c) 
}

\eq{
    (\vee) &: \Parser(A)\to\Parser(A)\to\Parser(A)\\
    p_1\vee p_2 &\defn\lambda s. \mathbf{case}\ p_1(s)\ \mathbf{of} \\
    & \Just(a, s)\mapsto \Just (a, s) \\
    & \Nothing \mapsto p_2(s). 
}

Now you can parse the digits and some other simple expressions.
$$
\name{seven}: \Parser(\nat)
$$
$$
\name{seven} \defn \bind(\pchar('7'),\lambda x.\unit(7))
$$

$$
\name{eight}: \Parser(\nat)
$$
$$
\name{eight} \defn \bind(\pchar('8'),\lambda x.\unit(8))
$$

$$
\name{plus}: \Parser(\nat\to\nat\to\nat)
$$
$$
\name{plus} \defn \bind(\pchar('+'),\lambda x.\unit(+))
$$

$$
\name{times}: \Parser(\nat\to\nat\to\nat)
$$
$$
\name{times} \defn \bind(\pchar('\times'),\lambda x.\unit(\times))
$$

$$
p_1: \Parser(\nat)
$$
$$
p_1 \defn (\name{seven}\vee\name{eight})
$$

$$
p_2: \Parser(\nat\to\nat\nat)
$$
$$
p_2 \defn (\name{plus}\vee\name{times})
$$

\eq{
    p_3&: \Parser(\nat) \\
    p_3 &\defn n_1 \leftarrow p_1 \\
    & op \leftarrow p_2 \\
    & n_2 \leftarrow p_1 \\
    &\unit(op(n_1, n_2))
}

Now play the $\beta$-reduction to $p_3(``7+8")$,
$p_3(``7\times7")$ or $p_3(``2+8")$.

The whole version of such a parser is better represented by a
programming language. You can find my version of it in Haskell
\url{https://github.com/fduxiao/haskellLearning/blob/master/parser/stateT.hs}.

\section{CPS Transformation - Generalization of Glivenko's theorem}
In the proof of \cref{bind-ipc}, we obtained a result of
type $\zero$. Since there's no constructors for $\zero$, that means
the type was inhabited by (in this case) function application, so
actually we can replace the $\zero$ with a general type $r$.

\newcommand{\Cont}{\name{Cont}}

\begin{definition}
    We define the type alias
    $$
        \Cont_r(A)\defn(A\to r)\to r
    $$
    together with
    $$
        \unit^r_A: A\to\Cont_r(A)
    $$
    $$
        \unit^r_A(a)\defn\lambda f. f(a)
    $$
    $$
    \bind^r_{A,B}: \Cont_r(A)\to(A\to\Cont_r(B))\to\Cont_r(B)
    $$
    $$
    \bind(ca, f) \defn \lambda k.
        ca(\lambda a.f(a)(k))
    $$
\end{definition}

For a calculation, we do not calculate its value immediately;
instead we encode it as given a {\it continuation} ($A\to r$), you
can apply the continuation to the result. If you pass $\id$ as the
continuation, then you can get the final result.

For example, we can define
$$
\name{three}\defn\unit(3)
$$

$$
\name{add}: \nat\to\nat\to\Cont(\nat)
$$
$$
\name{add}(x,y) \defn \unit(x+y)
$$

$$
\name{square}: \nat\to\Cont(\nat)
$$
$$
\name{square}(x) \defn \unit(x\times x)
$$

Now think what are
\eq{
    \name{pythagoras} &: \nat\to\nat\to\Cont(\nat) \\
    \name{pythagoras}(x,y) &\defn x^2 \leftarrow \name{square}(x) \\
        &y^2 \leftarrow \name{square}(y) \\
        &\name{add}(x^2,y^2)
}
and $\name{pythagoras}(3,4)(\id)$.

For a term $\bind(ca,f)$, where $ca: (A\to r)\to r$, the continuation
passed to $ca$ is $(\lambda a.f(a)(k))$, where $k$ is the continuation
of the result caculation. This means the calculating result is passed
to $f$, i.e. the bound variable will capture this value and the new
calculation $f(a): (B\to r)\to r$ will capture the next continuation. 

This looks like non-sense because it's totally unnecessary to define
something like this, but the problem is that with Curry-Howard
correspondence, the proof, i.e. the calculating process, is important. 
This style of constructing $\lambda$-terms is called {\it Continuation
Passing Style}. It helps to utter some complicated calculating process.

For example, we can define $t:(a,b)\mapsto a)$ and $f:(a,b)\mapsto b$,
then the expression $\mathbf{IF}\ c\ \mathbf{THEN}\ a\ \mathbf{ELSE}\ b$
is simply the application $c(a,b)$.

\newcommand{\callcc}{\name{callcc}}
More generally, there's a {\it Call with Current Continuation} function.
$$
\callcc: ((A\to\Cont(B))\to\Cont(A))\to\Cont(A)
$$
$$
\callcc(f) \defn \lambda k. f(\lambda a.\lambda k'. k(a))(k)
$$
(This is an analogue of Peirceâ€™s law $((p\to q)\to p)\to p$.) Note that
the $k'$ is the continuation used in $\Cont(B)$, but it is always
ignored.

For example, if you have the following monad
\eq{
    m &\defn name_1\leftarrow monad_1 \\
    &\cdots \\
    & name_n \leftarrow \callcc (\lambda exit. \\
        & \quad a_1 \leftarrow m_1 \\
        & \quad \cdots \\
        & \quad exit(somevalue) \\
        & \quad \cdots \\
    ) \\
    & \cdots \\
    & \unit(some\  result)
}

In the line of $\callcc$, it should give you a process of how to
make use of a continuation $k$. The function $exit: A\to \Cont(B)$
will apply this continuation $k$ to $somevalue$, ignoring the 
continuation $k'$ when $exit$ is called. 

In fact, think about the convariant version of Yoneda's embedding. 
Let $yA\defn \Pi_{r:\universe}A\to r$ be the covariant 
Hom-functor (it is well represented with $\lambda$-calculus only).
The Yoneda lemma tells us that for any functor $F$, we have 
$$
    \Hom[y(A), F] \cong FA,
$$
i.e.,
$$
    (\Pi_{r:\universe}(A\to r) \to Fr)\cong FA
$$
If $F$ is the identity functor, this is
$$
    (\Pi_{r:\universe}(A\to r)\to r)\cong A
$$

Though the above illustration is not strict because the Yoneda
lemma should be applied to a functor $\cat{C}\op\to\Sets$, so you
cannot assume $F$ is identity. It is not hard to prove a similar
theorem with pure type theory. 

The CPS transformation can be used as an intermediate representation
of computation \cite{appel2007compiling}. You can optimize or infer 
programs with it. For the logic part, the $\Ycomb$ combinator is also 
used with CPS in combinatory logic. I'm going to read \cite{Curry-Howard}
for this topic. 

Mathematically, a function is {\it pure}, i.e., you will always
expect a function give you the same result when it is applied. However,
this is not the real world case, for a human may input different stuff
at different times. The CPS transformation also helps in this situation,
i.e., we only know a user of a computer will input some data (of type
string, for example). Then we decide what happens to the data (a 
function from string to some other type $r$), which is exactly a 
continuation. No wonder some programming languages like javascript
choose CPS as its input/output tool. 
